{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN58lhPdXwwlFRhZrWCnLdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neel26desai/cmpe258_transfer_learning_cv/blob/main/Transfer_learning_with_video_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install remotezip tqdm  tf-models-official"
      ],
      "metadata": {
        "id": "vlogjasxafqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BstmzFc7Z3Hg"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files_per_class(zip_url):\n",
        "  \"\"\"\n",
        "    List the files in each class of the dataset given the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: URL from which the files can be unzipped.\n",
        "\n",
        "    Return:\n",
        "      files: List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(URL) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files\n",
        "\n",
        "def get_class(fname):\n",
        "  \"\"\"\n",
        "    Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Return:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]\n",
        "\n",
        "def get_files_per_class(files):\n",
        "  \"\"\"\n",
        "    Retrieve the files that belong to each class.\n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Return:\n",
        "      Dictionary of class names (key) and files (values).\n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class\n",
        "\n",
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\"\n",
        "    Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      to_dir: Directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file,)\n",
        "\n",
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\"\n",
        "    Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Return:\n",
        "      split_files: Files belonging to the subset of data.\n",
        "      remainder: Dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder\n",
        "\n",
        "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\"\n",
        "    Download a subset of the UFC101 dataset and split them into various parts, such as\n",
        "    training, validation, and test.\n",
        "\n",
        "    Args:\n",
        "      zip_url: Zip URL containing data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data\n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      dir: Posix path of the resulting directories containing the splits of data.\n",
        "  \"\"\"\n",
        "  files = list_files_per_class(zip_url)\n",
        "  for f in files:\n",
        "    tokens = f.split('/')\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "\n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    new_files_for_class = files_for_class[cls]\n",
        "    random.shuffle(new_files_for_class)\n",
        "    files_for_class[cls] = new_files_for_class\n",
        "\n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs\n",
        "\n",
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n",
        "\n",
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result\n",
        "\n",
        "class FrameGenerator:\n",
        "  def __init__(self, path, n_frames, training = False):\n",
        "    \"\"\" Returns a set of frames with their associated label.\n",
        "\n",
        "      Args:\n",
        "        path: Video file paths.\n",
        "        n_frames: Number of frames.\n",
        "        training: Boolean to determine if training dataset is being created.\n",
        "    \"\"\"\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths = list(self.path.glob('*/*.avi'))\n",
        "    classes = [p.parent.name for p in video_paths]\n",
        "    return video_paths, classes\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames)\n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ],
      "metadata": {
        "id": "TxB3HOf9feE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ufc_101_subset(URL,\n",
        "                        num_classes = 10,\n",
        "                        splits = {\"train\": 30, \"test\": 20},\n",
        "                        download_dir = download_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOK7V0PUab3c",
        "outputId": "52e86b56-65d3-4f42-fe5e-ca177b96d64a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [01:06<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:34<00:00,  5.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating train test split\n",
        "batch_size = 8\n",
        "num_frames = 8\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], num_frames, training = True),\n",
        "                                          output_signature = output_signature)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], num_frames),\n",
        "                                         output_signature = output_signature)\n",
        "test_ds = test_ds.batch(batch_size)"
      ],
      "metadata": {
        "id": "YsznBFBobZZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for frames, labels in train_ds.take(10):\n",
        "  print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4iEe9Sif_oh",
        "outputId": "f6f5a176-e968-40a3-9ec1-d7cc4feaf6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([9 6 5 9 9 8 0 7], shape=(8,), dtype=int16)\n",
            "tf.Tensor([8 5 8 9 1 6 8 6], shape=(8,), dtype=int16)\n",
            "tf.Tensor([7 8 1 5 2 4 5 7], shape=(8,), dtype=int16)\n",
            "tf.Tensor([5 8 0 8 8 8 1 4], shape=(8,), dtype=int16)\n",
            "tf.Tensor([4 7 9 5 3 0 9 5], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 7 3 2 3 1 4 9], shape=(8,), dtype=int16)\n",
            "tf.Tensor([0 0 6 3 4 9 4 9], shape=(8,), dtype=int16)\n",
            "tf.Tensor([1 2 4 9 5 6 4 7], shape=(8,), dtype=int16)\n",
            "tf.Tensor([3 9 3 6 7 1 9 9], shape=(8,), dtype=int16)\n",
            "tf.Tensor([2 9 0 0 7 1 1 9], shape=(8,), dtype=int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAdjwZhAgC8M",
        "outputId": "e7a55aed-8bc7-4471-c15e-52ea44961162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (8, 8, 224, 224, 3)\n",
            "Label: (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru = layers.GRU(units=4, return_sequences=True, return_state=True)\n",
        "\n",
        "inputs = tf.random.normal(shape=[1, 10, 8]) # (batch, sequence, channels)\n",
        "\n",
        "result, state = gru(inputs) # Run it all at once"
      ],
      "metadata": {
        "id": "UYdJJMaggFCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_half, state = gru(inputs[:, :5, :])   # run the first half, and capture the state\n",
        "second_half, _ = gru(inputs[:,5:, :], initial_state=state)  # Use the state to continue where you left off.\n",
        "\n",
        "print(np.allclose(result[:, :5,:], first_half))\n",
        "print(np.allclose(result[:, 5:,:], second_half))"
      ],
      "metadata": {
        "id": "CW3R-TTqgXMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Transfer learning for feature extraction\n",
        "\n",
        "\n",
        "We will be using movienet a0 model"
      ],
      "metadata": {
        "id": "zuFdjW-MpMP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'a0'\n",
        "resolution = 224\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPvJ-mSLgZlf",
        "outputId": "fcb182bd-c175-4da8-9af4-f0024aade2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movinet_a0_base/\n",
            "movinet_a0_base/checkpoint\n",
            "movinet_a0_base/ckpt-1.data-00000-of-00001\n",
            "movinet_a0_base/ckpt-1.index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7d97474b8c10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "1WWm05TegtUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 10)"
      ],
      "metadata": {
        "id": "ItkyNzZ7gtRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WFKBYZpGgtOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irWKZn7-hyf9",
        "outputId": "470289a9-3bd2-4783-aba1-3ffcadf40c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"movinet_classifier_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " image (InputLayer)          [(None, None, None, Non   0         \n",
            "                             e, 3)]                              \n",
            "                                                                 \n",
            " movinet (Movinet)           ({'stem': (None, None,    911583    \n",
            "                             None, None, 8),                     \n",
            "                              'block0_layer0': (None             \n",
            "                             , None, None, None, 8),             \n",
            "                              'block1_layer0': (None             \n",
            "                             , None, None, None, 32)             \n",
            "                             , 'block1_layer1': (Non             \n",
            "                             e, None, None, None, 32             \n",
            "                             ),                                  \n",
            "                              'block1_layer2': (None             \n",
            "                             , None, None, None, 32)             \n",
            "                             , 'block2_layer0': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block2_layer1': (None             \n",
            "                             , None, None, None, 56)             \n",
            "                             , 'block2_layer2': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block3_layer0': (None             \n",
            "                             , None, None, None, 56)             \n",
            "                             , 'block3_layer1': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block3_layer2': (None             \n",
            "                             , None, None, None, 56)             \n",
            "                             , 'block3_layer3': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block4_layer0': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'block4_layer1': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'block4_layer2': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'block4_layer3': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'head': (None, None, N             \n",
            "                             one, None, 480)},                   \n",
            "                              {'state_block0_layer0_             \n",
            "                             pool_buffer': (None, No             \n",
            "                             ne, None, None, 24),                \n",
            "                              'state_block0_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block1_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 80),                 \n",
            "                              'state_block1_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block1_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 80),                 \n",
            "                              'state_block1_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block1_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 80),                 \n",
            "                              'state_block1_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block2_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block2_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block2_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 112),                \n",
            "                              'state_block2_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block2_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block2_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer3_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer3_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 384),                \n",
            "                              'state_block4_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 280),                \n",
            "                              'state_block4_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 280),                \n",
            "                              'state_block4_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer3_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 344),                \n",
            "                              'state_block4_layer3_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_head_pool_buffe             \n",
            "                             r': (None, None, None,              \n",
            "                             None, 480),                         \n",
            "                              'state_head_pool_frame             \n",
            "                             _count': (1,)})                     \n",
            "                                                                 \n",
            " classifier_head_1 (Classif  (None, 10)                1005578   \n",
            " ierHead)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1917161 (7.31 MB)\n",
            "Trainable params: 1005578 (3.84 MB)\n",
            "Non-trainable params: 911583 (3.48 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt32lRUHgtLq",
        "outputId": "1ec4ae4a-f1af-4e02-9aa3-6d7d0c2fa814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "38/38 [==============================] - 122s 2s/step - loss: 0.9079 - accuracy: 0.7867 - val_loss: 0.1808 - val_accuracy: 0.9800\n",
            "Epoch 2/2\n",
            "38/38 [==============================] - 51s 1s/step - loss: 0.0885 - accuracy: 0.9833 - val_loss: 0.0853 - val_accuracy: 0.9900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "5g6l_T9iggem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ],
      "metadata": {
        "id": "UT31tAdHiOk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fg = FrameGenerator(subset_paths['train'], num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ],
      "metadata": {
        "id": "tIqtd0SyiI9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)\n",
        "plot_confusion_matrix(actual, predicted, label_names, 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCVR7sudiLRn",
        "outputId": "3e5824bb-ba51-4866-f4ea-dd92c8f082e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 34s 771ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using transfer learning for fine tuning use case"
      ],
      "metadata": {
        "id": "dz_i94wzpVpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backbone2 = movinet.Movinet(model_id=model_id)\n",
        "backbone2.trainable = True"
      ],
      "metadata": {
        "id": "716Mdh91iRvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2= build_classifier(batch_size, num_frames, resolution, backbone2, 10)"
      ],
      "metadata": {
        "id": "cBm6_wghiMmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXX9V0wLiufg",
        "outputId": "077ce97f-235e-40f8-c2dc-f721a0968f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"movinet_classifier_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " image (InputLayer)          [(None, None, None, Non   0         \n",
            "                             e, 3)]                              \n",
            "                                                                 \n",
            " movinet_1 (Movinet)         ({'stem': (None, None,    911583    \n",
            "                             None, None, 8),                     \n",
            "                              'block0_layer0': (None             \n",
            "                             , None, None, None, 8),             \n",
            "                              'block1_layer0': (None             \n",
            "                             , None, None, None, 32)             \n",
            "                             , 'block1_layer1': (Non             \n",
            "                             e, None, None, None, 32             \n",
            "                             ),                                  \n",
            "                              'block1_layer2': (None             \n",
            "                             , None, None, None, 32)             \n",
            "                             , 'block2_layer0': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block2_layer1': (None             \n",
            "                             , None, None, None, 56)             \n",
            "                             , 'block2_layer2': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block3_layer0': (None             \n",
            "                             , None, None, None, 56)             \n",
            "                             , 'block3_layer1': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block3_layer2': (None             \n",
            "                             , None, None, None, 56)             \n",
            "                             , 'block3_layer3': (Non             \n",
            "                             e, None, None, None, 56             \n",
            "                             ),                                  \n",
            "                              'block4_layer0': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'block4_layer1': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'block4_layer2': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'block4_layer3': (None             \n",
            "                             , None, None, None, 104             \n",
            "                             ),                                  \n",
            "                              'head': (None, None, N             \n",
            "                             one, None, 480)},                   \n",
            "                              {'state_block0_layer0_             \n",
            "                             pool_buffer': (None, No             \n",
            "                             ne, None, None, 24),                \n",
            "                              'state_block0_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block1_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 80),                 \n",
            "                              'state_block1_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block1_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 80),                 \n",
            "                              'state_block1_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block1_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 80),                 \n",
            "                              'state_block1_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block2_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block2_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block2_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 112),                \n",
            "                              'state_block2_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block2_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block2_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block3_layer3_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 184),                \n",
            "                              'state_block3_layer3_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer0_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 384),                \n",
            "                              'state_block4_layer0_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer1_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 280),                \n",
            "                              'state_block4_layer1_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer2_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 280),                \n",
            "                              'state_block4_layer2_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_block4_layer3_p             \n",
            "                             ool_buffer': (None, Non             \n",
            "                             e, None, None, 344),                \n",
            "                              'state_block4_layer3_p             \n",
            "                             ool_frame_count': (1,),             \n",
            "                              'state_head_pool_buffe             \n",
            "                             r': (None, None, None,              \n",
            "                             None, 480),                         \n",
            "                              'state_head_pool_frame             \n",
            "                             _count': (1,)})                     \n",
            "                                                                 \n",
            " classifier_head_4 (Classif  (None, 10)                1005578   \n",
            " ierHead)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1917161 (7.31 MB)\n",
            "Trainable params: 1902889 (7.26 MB)\n",
            "Non-trainable params: 14272 (55.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that when we se backbone as trainable the number of trainable parameters increases."
      ],
      "metadata": {
        "id": "l7sPWdr-p4Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results2 = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=4,\n",
        "                    initial_epoch=2,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68H1rT-ljcx_",
        "outputId": "5bbfe7d7-bfca-4228-86f9-e7cddb01b5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/4\n",
            "38/38 [==============================] - 52s 1s/step - loss: 0.0437 - accuracy: 0.9900 - val_loss: 0.0851 - val_accuracy: 0.9850\n",
            "Epoch 4/4\n",
            "38/38 [==============================] - 58s 2s/step - loss: 0.0362 - accuracy: 0.9933 - val_loss: 0.0653 - val_accuracy: 0.9750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "acc = results.history['accuracy']\n",
        "val_acc = results.history['val_accuracy']\n",
        "\n",
        "loss = results.history['loss']\n",
        "val_loss = results.history['val_loss']\n",
        "\n",
        "\n",
        "acc += results2.history['accuracy']\n",
        "val_acc += results2.history['val_accuracy']\n",
        "\n",
        "loss += results2.history['loss']\n",
        "val_loss += results2.history['val_loss']"
      ],
      "metadata": {
        "id": "yfSlynVIlJmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZFDaTxjoXXi",
        "outputId": "d74917d7-3e28-4060-beb1-aeaa4886a5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9800000190734863,\n",
              " 0.9900000095367432,\n",
              " 0.9850000143051147,\n",
              " 0.9750000238418579]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "\n",
        "1. https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub"
      ],
      "metadata": {
        "id": "DkzhXY0IrpaB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SN8YiCrYp1TY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}